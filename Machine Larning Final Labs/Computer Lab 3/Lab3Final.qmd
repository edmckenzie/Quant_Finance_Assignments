---
title: "Lab3 Q 4/5"
author: 
  - Edward Mckenzie
date: last-modified
format: 
  html:
    self-contained: true
toc: true
execute:
  error: false
language: 
  title-block-author-single: " "
theme: Default
title-block-banner-color: Primary
editor: visual
---

## Q4

#### 4.1

```{r}

pairwise_cov_squared_exp <- function(x, x_prime, sigma_f, ell) {
  return(sigma_f^2*exp(-1/(2*ell^2)*(x - x_prime)^2))
}

covariance1 <- pairwise_cov_squared_exp(0.3, 0.7, 1.5, 0.5)
covariance2 <- pairwise_cov_squared_exp(0.1, 0.5, 1.5, 0.5)
correlation3 <- pairwise_cov_squared_exp(-0.2, -0.5, 1.5, 0.5) / sqrt(pairwise_cov_squared_exp(-0.2, -0.2, 1.5, 0.5) * pairwise_cov_squared_exp(-0.5, -0.5, 1.5, 0.5))

cat("\nCovariance between 0.3 and 0.7: ", covariance1)
cat("\nCovariance between 0.1 and 0.5: ", covariance2)
cat("\nCorrelation between -0.2 and -0.5: ", correlation3)
```

The covariances in 1 and 2 are then same because our kernel function is a stationary kernel. This means that the covariance is a function of the relative distance between points, not their location. Since 1 and 2 both have distances of 0.4 thew calculated covariances are the same.

#### 4.2

```{r}

X <- seq(-1, 1, length.out = 21)

covariance_matrix <- function(X) {

Kernel_Mat <- matrix(0, length(X), length(X))

for (i in 1:length(X)) {
  for (j in 1:length(X)) {
    Kernel_Mat[i, j] = pairwise_cov_squared_exp(X[i], X[j], 1.5, 0.5)
  }
}
return(Kernel_Mat)
}

result <- covariance_matrix(X)

print(result[2,5])
```

The value at row 2 column 5 represents the covariance value between the 2nd and 5th point in our X sequence as calculated using the squared exponential kernel function

#### 4.3

```{r}

X<-seq(-1,1,length.out=500)

kernel_matrix_squared_exp <- function(X, Xstar, sigma_f, ell) {
  # Computes the kernel matrix for the squared exponential kernel model
  # Compute the pairwise squared Euclidean distances
  pairwise_squared_distances <- outer(X, Xstar, FUN = "-")^2
  # Compute the kernel matrix element-wise
  kernel_matrix <- sigma_f^2*exp(-1/(2*ell^2)*pairwise_squared_distances)
  return(kernel_matrix)
}

library(rbenchmark)

sigma_f = 1.5
ell = 0.5

comparison = benchmark(
    "loop" = covariance_matrix(X)  ,
    "vectorised" = kernel_matrix_squared_exp(X, X, sigma_f, ell),
    replications = 15,
    columns = c("test" , "elapsed", "relative"),
    order = "elapsed"
)

print(comparison)
```

#### 4.4

```{r}

suppressMessages(library(mvtnorm)) # for multivariate normal
n_grid <- 200
X_grid <- seq(-1, 1, length.out = n_grid)
sigma_f <- 1.5
ell <- 0.5
m_X <- rep(0, n_grid) # Create zero vector
K_X_X <- kernel_matrix_squared_exp(X_grid, X_grid, sigma_f, ell)
GP_realisations <- rmvnorm(n = 5, mean = m_X, sigma = K_X_X)
matplot(X_grid, t(GP_realisations), type = "l", lty = 1, col = c("cornflowerblue", "lightcoral", "green", "black", "purple"), xlab = "x", ylab = "f(x)", main = "Simulations from the GP prior", xlim=c(-1, 1.5), ylim=c(-3*sigma_f, 3*sigma_f))
legend("topright", legend = c("Sim 1", "Sim 2", "Sim 3", "Sim 4", "Sim 5"), col = c("cornflowerblue", "lightcoral", "green", "black", "purple"), lty = 1)


matplot(X_grid, t(GP_realisations), type = "l", lty = 1, col = c("cornflowerblue", "lightcoral", "green", "black", "purple"), xlab = "x", ylab = "f(x)", main = "Simulations from the GP prior", xlim=c(-1, 1.5), ylim=c(-3*sigma_f, 3*sigma_f))
legend("topright", legend = c("Sim 1", "Sim 2", "Sim 3", "Sim 4", "Sim 5"), col = c("cornflowerblue", "lightcoral", "green", "black", "purple"), lty = 1)

# Define x values for shading (X_grid for this example)
x_shade <- X_grid
# Lower and upper interval (prior mean is zero)
lower_interval <- -1.96*sigma_f*rep(1, n_grid)
upper_interval <- 1.96*sigma_f*rep(1, n_grid)

# Create a polygon to shade the prediction interval (alpha controls transparency)
polygon(c(x_shade, rev(x_shade)), c(lower_interval, rev(upper_interval)), col = rgb(0, 0, 1, alpha = 0.05), border = NA)
```

The length-scale hyperparameter controls the correlation length between datapoints. As we decrease the value of l, two neighbouring points have to be explained by a sharper variation in the value of the underlying function due to this correlation distance being smaller. This has the effect of creating a wigglier function therefore increasing the variance and lowering the bias of the model. The opposite is true when we increase the value of l i.e smoother function, higher bias, lower variance.

## Q5

#### 5.1

$$
y = f(x) + \epsilon
$$

Where:

$$
f(x) \sim GP(m(x), k(x,x\prime)) \space \text{and} \space \epsilon \sim N(0, \sigma_\epsilon^2)
$$

Therefore, taking the expectation of y under the measure/filtration generated by f:

$$
\mathbb{E}(y) = \mathbb{E}_\mathbf{f}\left(\mathbb{E}\left(\mathbf{y}|\mathbf{f}\right)\right) = \mathbb{E}_\mathbf{f}(\mathbb{E}(f(x) + \epsilon) |f) 
$$

By linearity of expectations:

$$
\mathbb{E}_\mathbf{f}(\mathbb{E}(f(x) + \epsilon) |f) = \mathbb{E}_\mathbf{f}(\mathbb{E}(f(x)|f) + \mathbb{E}_\mathbf{f}(\mathbb{E}(\epsilon |f)) = \mathbb{E}_\mathbf{f}(f(x)) + \mathbb{E}_\mathbf{f}(0) = m(X) + 0 = m(X)
$$

Therefore:

$$
\mathbb{E}(y) = \mathbb{E}_\mathbf{f}\left(\mathbb{E}\left(\mathbf{y}|\mathbf{f}\right)\right) = m(X)
$$

For the Covariance:

$$
\mathrm{Cov}\left(\mathbf{y}\right)= \mathbb{E}_\mathbf{f}\left(\mathrm{Cov}\left(\mathbf{y}|\mathbf{f}\right)\right)+\mathrm{Cov}_\mathbf{f}\left(\mathbb{E}\left(\mathbf{y}|\mathbf{f}\right)\right)
$$

First term:

$$
\mathbb{E}_\mathbf{f}\left(\mathrm{Cov}\left(\mathbf{y}|\mathbf{f}\right)\right) = \mathbb{E}_\mathbf{f}\left(\mathrm{Cov}\left(f(x) + \epsilon|\mathbf{f}\right)\right)
$$

Inside the expectation, noting these are vector values functions:

$$
\mathrm{Cov}\left(f(x) + \epsilon|\mathbf{f}\right) = \mathrm{Cov}\left(f(x) ,f(x)|\mathbf{f}\right) + \mathrm{Cov}\left(f(x),\epsilon|\mathbf{f}\right) + \mathrm{Cov}\left(\epsilon, f(x)|\mathbf{f}\right) + \mathrm{Cov}\left(\epsilon, \epsilon|\mathbf{f}\right)
$$

The first 3 terms on the right are all equal to zero as we are conditioning on f, the final term is then:

$$
\mathrm{Cov}\left(\epsilon, \epsilon|\mathbf{f}\right) = \mathrm{Var}(\epsilon | \mathbf{f})
$$

Therefore:

$$
\mathbb{E}_\mathbf{f}\left(\mathrm{Cov}\left(\mathbf{y}|\mathbf{f}\right)\right) = \mathbb{E}_\mathbf{f}\left(\mathrm{Cov}\left(f(x) + \epsilon|\mathbf{f}\right)\right) = \mathbb{E}_\mathbf{f}\left(\mathrm{Var}(\epsilon | \mathbf{f}\right)) = \sigma_\epsilon^2 \mathbf{I_n}
$$

Second term:

$$
\mathrm{Cov}_\mathbf{f}\left(\mathbb{E}\left(\mathbf{y}|\mathbf{f}\right)\right) = \mathrm{Cov}_\mathbf{f}\left(\mathbb{E}\left(f(x) + \epsilon|\mathbf{f}\right)\right) = \mathrm{Cov}_\mathbf{f}\left(\mathbb{E}\left(f(x) |\mathbf{f}\right) + \mathbb{E}\left(\epsilon |\mathbf{f}\right) \right) = \mathrm{Cov}_\mathbf{f}(f(x) + \mathbb{E}\left(\epsilon |\mathbf{f}\right), f(x) + \mathbb{E}\left(\epsilon |\mathbf{f}\right))
$$

We know:

$$
\mathbb{E}\left(\epsilon |\mathbf{f}\right) = 0
$$

Therefore:

$$
\mathrm{Cov}_\mathbf{f}(f(x) + \mathbb{E}\left(\epsilon |\mathbf{f}\right), f(x) + \mathbb{E}\left(\epsilon |\mathbf{f}\right)) = \mathrm{Cov}_\mathbf{f}(f(x) , f(x)) = K(X,X)
$$

Adding the two terms together we get:

$$
\mathrm{Cov}\left(\mathbf{y}\right)= K(X,X) + \sigma_\epsilon^2 \mathbf{I_n}
$$

#### 5.2

```{r}
load(file = 'penguins.RData')
y <- penguins$dive_heart_rate
n <- length(y)
X <- penguins$duration/max(penguins$duration) # Scale duration [0, 1]
plot(X, y, main="DHR vs scaled duration", col = "cornflowerblue", xlab = "Scaled duration", ylab = "Dive heart rate (DHR)")
sigma_f <- 100
ell <- 0.6
sigma_eps <- sqrt(150)
X_star <- X # For smoothing
# Compute means and kernel matrices (smoothing case can reuse computations)
# Prior means
m_X <- rep(0, n)
m_Xstar <- m_X
# Prior covariances
K_X_X <- kernel_matrix_squared_exp(X, X, sigma_f, ell)
K_X_Xstar <- K_X_X
K_Xstar_X <- K_X_Xstar
K_Xstar_Xstar <- K_X_X
# Conditional distribution of f given y is normal. 
fbar_star <- m_Xstar + K_Xstar_X%*%solve(K_X_X + sigma_eps^2*diag(n), y - m_X)
lines(X, fbar_star, col = "lightcoral", type = "p")
legend(x = "topright", pch = c(1, 1), col = c("cornflowerblue", "lightcoral"), legend=c("Data", "Smoothed (fitted) values"))
```

```{r}

x_grid <- seq(0,1,length.out=1000)

K <- kernel_matrix_squared_exp(X, X, sigma_f, ell) + sigma_eps^2 * diag(n)
K_x <- kernel_matrix_squared_exp(X, x_grid, sigma_f, ell)
K_xx <- kernel_matrix_squared_exp(x_grid, x_grid, sigma_f, ell)

mu_x <- t(K_x) %*% solve(K) %*% y
cov_x <- K_xx - t(K_x) %*% solve(K) %*% K_x

std_x <- sqrt(diag(cov_x))

upper_bound <- mu_x + 1.96*std_x
lower_bound <- mu_x - 1.96*std_x

plot(X, y, main = "DHR with GP Predictions", col = "cornflowerblue", xlab = "Scaled duration", ylab = "Dive heart rate (DHR)")
lines(x_grid, mu_x, col = "lightcoral")
lines(x_grid, lower_bound, col = "lightcoral", lty = 4)
lines(x_grid, upper_bound, col = "lightcoral", lty = 4)
legend(x = "topright", pch = c(1, 1, -1, -1), col = c("cornflowerblue", "lightcoral", "lightcoral", "lightcoral"), legend=c("Data", "Posterior mean", "Lower 95%", "Upper 95%"), lty=c(NA, NA, 2, 2))

```

Some potential reasons as to why the confidence interval generated by the GP doesn't include 95% of the data:

1.  Our generated data comes from a very different distribution compared to the dataset. As we are just creating a uniform sample from 0 - 1 there is no reason to expect the data distributions to be similar
2.  The parameters of our kernel have not been optimised to fit the data appropriately

#### 5.3

```{r}

fn <- function(l, X_train, y_train, noise) {
  n <- length(X_train)
  K <- kernel_matrix_squared_exp(X_train, X_train, 100, l) + noise^2 * diag(n)
  
  log_marginal_likelihood <- - 0.5 * t(y_train) %*% solve(K) %*% y_train - 0.5 * log(det(K)) - 0.5 * n * log(2*pi)
  
  return(-log_marginal_likelihood)
  
}

optimal_length <- optim(par = ell, fn=fn, X_train = X, y_train = y, noise = sigma_eps, lower = 0, upper = Inf, method = "L-BFGS-B")

optimal_ell <- optimal_length$par

cat("\n The optimal ell value is: ", optimal_ell)

```

#### 5.4

```{r}

ell_grid<-seq(0,1,length.out=50)

folds <- cut(seq(1, n),breaks=5,labels=FALSE)

posterior <- function(X_s, X_train, y_train, l, variance, noise) {
  n <- length(X_train)
  K <- kernel_matrix_squared_exp(X_train, X_train, variance, l) + noise^2 * diag(n)
  K_x <- kernel_matrix_squared_exp(X_train, X_s, variance, l)
  K_xx <- kernel_matrix_squared_exp(X_s, X_s, variance, l)
  
  mu_x <- t(K_x) %*% solve(K) %*% y_train
  
  return(mu_x)
  
}

data <- cbind(X, y)

RMSE <- matrix(NA, 5, 50)

for(i in 1:5){
   
  testIndexes <- which(folds==i, arr.ind=TRUE)
  testData <- data[testIndexes, ]
  trainData <- data[-testIndexes, ]
  
  rmse <- c()
  
  for(l in 1:length(ell_grid)) {
    
    pred <- posterior(testData[, 1], trainData[, 1], trainData[, 2], ell_grid[l], sigma_f, sigma_eps)
    
    rmse[l] <- sqrt(mean(pred - testData[, 2])^2)
    
    
  }
  
  RMSE[i, ] <- rmse
  
}

average_RMSE <- apply(RMSE, 2, mean)

optimal_ell_ind <- which.min(average_RMSE) 

optimal_cv_ell <- ell_grid[optimal_ell_ind] 

cat("\nOptimal ell value from CV: ", optimal_cv_ell)
```

#### 5.5

```{r}

fn2 <- function(theta, X_train, y_train) {
  n <- length(X_train)
  l <- theta[1]
  variance <- theta[2]
  noise <- theta[3]
  
  K <- kernel_matrix_squared_exp(X_train, X_train, variance, l) + noise^2 * diag(n)
  
  log_marginal_likelihood <- - 0.5 * t(y_train) %*% solve(K) %*% y_train - 0.5 * log(det(K)) - 0.5 * n * log(2*pi)
  
  return(-log_marginal_likelihood)
  
}

optimal_params <- optim(par = c(ell, sigma_f, sigma_eps), fn=fn2, X_train = X, y_train = y, lower = c(0,0,0) , upper = c(Inf, Inf, Inf), method = "L-BFGS-B")

optimal_ell_multi <- optimal_params$par[1]
optimal_var_multi <- optimal_params$par[2]
optimal_noise_multi <- optimal_params$par[3]

cat("\nOptimal ell form mulitvariable optimisation: ", optimal_ell_multi)
cat("\nOptimal variance from multivariable optimisation: ", optimal_var_multi)
cat("\nOptimal noise from multivariable optimisation: ", optimal_noise_multi)
```

The estimates from the multivariable optimisation process are close to the values that were given initially
